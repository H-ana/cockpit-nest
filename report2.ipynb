{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ObkZth5YP5OH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "from bokeh.models import ColumnDataSource, HoverTool\n",
        "from pyspark.sql.functions import datediff,current_date,to_date,date_format,count,col,when\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType, TimestampType, DoubleType\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "60TEaxl3P5OJ"
      },
      "outputs": [],
      "source": [
        "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iMJ7Kp3lP5OL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/home/user/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /home/user/.ivy2/cache\n",
            "The jars for the packages stored in: /home/user/.ivy2/jars\n",
            "org.apache.hadoop#hadoop-aws added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-01ef5ca5-ca85-48cc-b094-6637d35da666;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.hadoop#hadoop-aws;3.0.0 in central\n",
            "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.199 in central\n",
            ":: resolution report :: resolve 208ms :: artifacts dl 7ms\n",
            "\t:: modules in use:\n",
            "\tcom.amazonaws#aws-java-sdk-bundle;1.11.199 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-aws;3.0.0 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-01ef5ca5-ca85-48cc-b094-6637d35da666\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/01/14 22:13:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/01/14 22:13:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.0.0\") \\\n",
        "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
        "        .config(\"spark.hadoop.fs.s3a.access.key\", 'AKIA3AEXDSNEGXQERCGG') \\\n",
        "        .config(\"spark.hadoop.fs.s3a.secret.key\", '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') \\\n",
        "        .appName('Report 2 : SC Supply Chain Report')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "roXOe-y9P5ON"
      },
      "outputs": [],
      "source": [
        "itemSchema = StructType([\n",
        "    StructField(\"No\", StringType(), True),\n",
        "    StructField(\"No_ 2\", StringType(), True),\n",
        "    StructField(\"Description\", StringType(), True),\n",
        "    StructField(\"Search Description\", StringType(), True),\n",
        "    StructField(\"Description 2\", StringType(), True),\n",
        "    StructField(\"Base Unit of Measure\", StringType(), True),\n",
        "    StructField(\"Price Unit Conversion\", StringType(), True),\n",
        "    StructField(\"Type\", StringType(), True),\n",
        "    StructField(\"Inventory Posting Group\", StringType(), True),\n",
        "    StructField(\"Shelf No_\", StringType(), True),\n",
        "    StructField(\"Item Disc_ Group\", StringType(), True),\n",
        "    StructField(\"Allow Invoice Disc_\", StringType(), True),\n",
        "    StructField(\"Statistics Group\", StringType(), True),\n",
        "    StructField(\"Commission Group\", StringType(), True),\n",
        "    StructField(\"Unit Price\", IntegerType(), True),\n",
        "    StructField(\"Price_Profit Calculation\", StringType(), True),\n",
        "    StructField(\"Profit _\", StringType(), True),\n",
        "    StructField(\"Costing Method\", StringType(), True),\n",
        "    StructField(\"Unit Cost\", StringType(), True),\n",
        "    StructField(\"Standard Cost\", StringType(), True),\n",
        "    StructField(\"Quoted Price(INR)\", StringType(), True),\n",
        "    StructField(\"Quoted Price(FCY)\", StringType(), True),\n",
        "    StructField(\"Quoted Currency\", StringType(), True),\n",
        "    StructField(\"Standard Cost_\", StringType(), True),\n",
        "    StructField(\"Production_BOM_No\", StringType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8WFeCSkaP5OP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "item_df = spark.read.parquet(\"s3a://hackathon2023/data/SCSupplyChain/item/item.parquet\", inferSchema=True)\n",
        "bronze_item_df = item_df\n",
        "for col in item_df.columns:\n",
        "    item_df = item_df.withColumnRenamed(col, [f.name for f in itemSchema.fields if f.name != col][0])\n",
        "item_df = spark.createDataFrame(item_df.rdd, itemSchema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FLuAwXuIGx6Y"
      },
      "outputs": [],
      "source": [
        "item_df=item_df.drop('No_ 2')\n",
        "item_df=item_df.drop('Description 2')\n",
        "item_df=item_df.drop('Search Description')\n",
        "item_df=item_df.drop('Type')\n",
        "item_df=item_df.na.drop()\n",
        "silver_item_df = item_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ne6sPJJPP5OQ"
      },
      "outputs": [],
      "source": [
        "warehouseSchema=StructType([\n",
        "    StructField(\"Entry No\", StringType(), True),\n",
        "    StructField(\"Journal Batch Name\", StringType(), True),\n",
        "    StructField(\"Line No_\", StringType(), True),\n",
        "    StructField(\"Registering Date\", StringType(), True),\n",
        "    StructField(\"Location Code\", StringType(), True),\n",
        "    StructField(\"Zone Code\", StringType(), True),\n",
        "    StructField(\"Bin Code\", StringType(), True),\n",
        "    StructField(\"Description\", StringType(), True),\n",
        "    StructField(\"Item No_\", StringType(), True),\n",
        "    StructField(\"Quantity\", StringType(), True),\n",
        "    StructField(\"Qty_ (Base)\", StringType(), True),\n",
        "    StructField(\"Source Type\", StringType(), True),\n",
        "    StructField(\"Source Subtype\", StringType(), True),\n",
        "    StructField(\"Source No_\", StringType(), True),\n",
        "    StructField(\"Source Line No_\", StringType(), True),\n",
        "    StructField(\"Source Subline No_\", StringType(), True),\n",
        "    StructField(\"Source Document\", StringType(), True),\n",
        "    StructField(\"Source Code\", StringType(), True),\n",
        "    StructField(\"Reason Code\", StringType(), True),\n",
        "    StructField(\"No_ Series\", StringType(), True),\n",
        "    StructField(\"Bin Type Code\", StringType(), True),\n",
        "    StructField(\"Cubage\", StringType(), True),\n",
        "    StructField(\"Weight\", StringType(), True),\n",
        "    StructField(\"Journal Template Name\", StringType(), True),\n",
        "    StructField(\"Whse_ Document No_\", StringType(), True),\n",
        "    StructField(\"Whse_ Document Type\", StringType(), True),\n",
        "    StructField(\"Whse_ Document Line No_\", StringType(), True),\n",
        "    StructField(\"Entry Type\", StringType(), True),\n",
        "    StructField(\"Reference Document\", StringType(), True),\n",
        "    StructField(\"Reference No_\", StringType(), True),\n",
        "    StructField(\"User ID\", StringType(), True),\n",
        "    StructField(\"Variant Code\", StringType(), True),\n",
        "    StructField(\" Qty_ per Unit of Measure\", StringType(), True),\n",
        "    StructField(\"Unit of Measure Code\", StringType(), True),\n",
        "    StructField(\"Serial No_\", StringType(), True),\n",
        "    StructField(\"Lot No_\", StringType(), True),\n",
        "    StructField(\"Warranty Date\", StringType(), True),\n",
        "    StructField(\"Expiration Date\", StringType(), True),\n",
        "    StructField(\"Phys Invt Counting Period Code\", StringType(), True),\n",
        "    StructField(\"Phys Invt Counting Period Type\", StringType(), True),\n",
        "    StructField(\"Dedicated\", StringType(), True),\n",
        "    StructField(\"Company\", StringType(), True),\n",
        "    StructField(\"Division\", StringType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3djQEnOOP5OR"
      },
      "outputs": [],
      "source": [
        "warehouse_df=spark.read.format(\"csv\").option(\"header\", \"false\").option(\"delimiter\",\",\").schema(warehouseSchema).load(\"s3a://hackathon2023/data/SCSupplyChain/warehouse/warehouse.csv\")\n",
        "bronze_warehousedf = warehouse_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DZqT2TLzP5OS"
      },
      "outputs": [],
      "source": [
        "warehouse_df = warehouse_df.withColumn(\"Registering Date\", to_date(warehouse_df[\"Registering Date\"], \"dd-MM-yyyy\"))\n",
        "warehouse_df=warehouse_df.drop('Journal Batch Name')\n",
        "warehouse_df=warehouse_df.drop('Reason Code')\n",
        "warehouse_df=warehouse_df.drop('Journal Template Name')\n",
        "warehouse_df=warehouse_df.drop('Variant Code')\n",
        "warehouse_df=warehouse_df.drop('Serial No_')\n",
        "warehouse_df=warehouse_df.drop('Company')\n",
        "warehouse_df=warehouse_df.drop('Division')\n",
        "warehouse_df=warehouse_df.drop('Qty_ (Base)')\n",
        "silver_warehousedf = warehouse_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dwcMKAKwP5OU"
      },
      "outputs": [],
      "source": [
        "production_df=spark.read.format(\"csv\").option(\"header\",\"True\").option(\"delimiter\",\"\\t\").load(\"s3a://hackathon2023/data/SCSupplyChain/production/production.txt\")\n",
        "bronze_productiondf = production_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zLkAwvXFGvCG"
      },
      "outputs": [],
      "source": [
        "production_df=production_df.drop('Version Code')\n",
        "production_df=production_df.drop('Position 2')\n",
        "production_df=production_df.drop('Position 3')\n",
        "production_df=production_df.drop('Company')\n",
        "production_df=production_df.drop('Division')\n",
        "production_df=production_df.na.drop()\n",
        "silver_productiondf = production_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9Mhn5ZXWP5OU"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession, functions as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UbyCJPjJP5OU"
      },
      "outputs": [],
      "source": [
        "df_1 = warehouse_df.groupBy(\"Lot No_\", \"Bin Code\", \"Item No_\",\"Registering Date\").agg(\n",
        "    F.min(\"Registering Date\").alias(\"min_registering_date\"),\n",
        "    F.sum(\"Quantity\").alias(\"sum_quantity\"),\n",
        "    F.first(\"Zone Code\").alias(\"first_zone_code\"),\n",
        "    F.datediff(F.current_date(), F.col(\"Registering Date\")).alias(\"date_diff\")\n",
        ").filter(\"sum_quantity > 0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sfkPhXfmP5OU"
      },
      "outputs": [],
      "source": [
        "df_2 = item_df.filter(\"Production_BOM_No != ''\").select(\"No\", \"Production_BOM_No\").union(\n",
        "    production_df.select(\"No_\", \"Production BOM No_\")\n",
        ").distinct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hlXeJ0wIP5OU"
      },
      "outputs": [],
      "source": [
        "df_3 = df_1.join(df_2, df_1[\"Item No_\"] == df_2[\"No\"], \"left\").drop(\"No\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YT8AhioUP5OV"
      },
      "outputs": [],
      "source": [
        "df_3=df_3.drop('Production_BOM_No')\n",
        "df_4 = df_3.join(item_df, df_3[\"Item No_\"] == item_df[\"No\"], \"inner\")\n",
        "df_4=df_4.drop('No')\n",
        "df_4=df_4.drop('min_registering_date')\n",
        "df_4=df_4.drop('date_diff')\n",
        "df_4=df_4.drop('first_zone_code')\n",
        "gold_df = df_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT9KuZdfP5OV",
        "outputId": "d2361839-c096-4444-d827-fb0550b7e77f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+---------------+\n",
            "|Item Disc_ Group|Inventory Value|\n",
            "+----------------+---------------+\n",
            "|            D666|      584067176|\n",
            "|            D668|      554531823|\n",
            "|            D655|      628101653|\n",
            "|            D654|      573952121|\n",
            "|            D667|      559958218|\n",
            "+----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inventory_value_by_category = df_4.groupBy(\"Item Disc_ Group\").agg(F.sum(\"Unit Price\").alias(\"Inventory Value\")) \n",
        "inventory_value_by_category.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdKzsCjXP5OX",
        "outputId": "a3951afc-915f-40f4-c55a-37f2d4b01bec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 22:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------+---------------+\n",
            "|Item Disc_ Group|Inventory Value|\n",
            "+----------------+---------------+\n",
            "|            D655|      628101653|\n",
            "|            D666|      584067176|\n",
            "|            D654|      573952121|\n",
            "|            D667|      559958218|\n",
            "|            D668|      554531823|\n",
            "+----------------+---------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "inventory_value_by_category = df_4.groupBy(\"Item Disc_ Group\").agg(F.sum(\"Unit Price\").alias(\"Inventory Value\"))\n",
        "top_10_categories = inventory_value_by_category.sort(\"Inventory Value\", ascending=False).limit(10)\n",
        "top_10_categories.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbDWU440P5OY",
        "outputId": "79546fa9-cf79-4f15-a673-f1f91e610af3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 37:>                                                         (0 + 1) / 1]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---------------+\n",
            "|Age|Inventory Value|\n",
            "+---+---------------+\n",
            "|148|        5847758|\n",
            "|463|        4412744|\n",
            "|471|        5719729|\n",
            "|496|        2349978|\n",
            "|392|        4943068|\n",
            "|243|        4494924|\n",
            "|540|        4957442|\n",
            "|623|        5487348|\n",
            "| 31|        4625850|\n",
            "|516|        3941522|\n",
            "|251|        2349298|\n",
            "| 85|        3919500|\n",
            "|137|        5340050|\n",
            "|451|        1897912|\n",
            "|580|        2925022|\n",
            "|458|        5198470|\n",
            "| 65|        1037478|\n",
            "|481|        5460234|\n",
            "|588|        2834322|\n",
            "|255|        4227766|\n",
            "+---+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_4 = df_4.withColumn(\"Age\", datediff(current_date(), to_date(\"Registering Date\", \"yyyy-MM-dd\")))\n",
        "inventory_value_by_age = df_4.groupBy(\"Age\").agg(F.sum(\"Unit Price\").alias(\"Inventory Value\"))\n",
        "inventory_value_by_age.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYdIgZjPP5OZ",
        "outputId": "4be749bd-e015-47df-e9b1-7cf4d18a3641"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+----------+\n",
            "|Bin Code|     Value|\n",
            "+--------+----------+\n",
            "|     TMG|1668916626|\n",
            "|     IGS|1231694365|\n",
            "+--------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bin_value = df_4.groupBy(\"Bin Code\").agg(F.sum(\"Unit Price\").alias(\"sum(value)\")).withColumnRenamed(\"sum(value)\", \"Value\")\n",
        "bin_value.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7 (main, Nov 24 2022, 19:45:47) [GCC 12.2.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

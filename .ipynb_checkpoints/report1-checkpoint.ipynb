{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType, TimestampType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/user/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/user/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cad571ac-a225-4041-8533-4517c8cfdd07;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.0.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.199 in central\n",
      ":: resolution report :: resolve 152ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.199 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.0.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cad571ac-a225-4041-8533-4517c8cfdd07\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/7ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/10 11:26:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/10 11:26:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.0.0\") \\\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", 'AKIA3AEXDSNEGXQERCGG') \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", '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') \\\n",
    "        .appName('Report 1 : Operations Management Report')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsSchema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"BoardId\", IntegerType(), True),\n",
    "    StructField(\"BatchId\", StringType(), True),\n",
    "    StructField(\"WorkOrderId\", StringType(), True),\n",
    "    StructField(\"RoutingStageId\", StringType(), True),\n",
    "    StructField(\"RoutingStageName\", StringType(), True),\n",
    "    StructField(\"Operator\", StringType(), True),\n",
    "    StructField(\"Deviation\", StringType(), True),\n",
    "    StructField(\"InspectionDate\", StringType(), True),\n",
    "    StructField(\"LastModifiedDate\", StringType(), True),\n",
    "    StructField(\"ReInspectionNeeded\", StringType(), True),\n",
    "    StructField(\"PreviouslySannedBoards\", StringType(), True),\n",
    "    StructField(\"RoutingStatus\", StringType(), True),\n",
    "    StructField(\"CavityID\", StringType(), True),\n",
    "    StructField(\"SubWorkCenter\", StringType(), True),\n",
    "    StructField(\"StationCode\", StringType(), True),\n",
    "    StructField(\"StationName\", StringType(), True),\n",
    "    StructField(\"TrayId\", StringType(), True),\n",
    "    StructField(\"AssetSubNodeId\", StringType(), True),\n",
    "    StructField(\"CollectionId\", StringType(), True),\n",
    "    StructField(\"Company\", StringType(), True),\n",
    "    StructField(\"Division\", StringType(), True),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "workOrdersSchema=StructType([\n",
    "    StructField(\"Id\", StringType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True),\n",
    "    StructField(\"LineNo\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", StringType(), True),\n",
    "    StructField(\"Started\", StringType(), True),\n",
    "    StructField(\"StartDate\", StringType(), True),\n",
    "    StructField(\"EndDate\", StringType(), True),\n",
    "    StructField(\"EcnNo\", StringType(), True),\n",
    "    StructField(\"EcnQunatity\", StringType(), True),\n",
    "    StructField(\"EcnStatus\", StringType(), True),\n",
    "    StructField(\"ProductRevision\", StringType(), True),\n",
    "    StructField(\"PlannedStartDate\", StringType(), True),\n",
    "    StructField(\"PlannedEndDate\", StringType(), True),\n",
    "    StructField(\"Isblocked\", StringType(), True),\n",
    "    StructField(\"BlockedDate\", StringType(), True),\n",
    "    StructField(\"BlockedBy\", StringType(), True),\n",
    "    StructField(\"BatchProceedStatus\", StringType(), True),\n",
    "    StructField(\"WorkOrderClosureStatus\", StringType(), True),\n",
    "    StructField(\"ShortClosedQuantity\", StringType(), True),\n",
    "    StructField(\"CreationDate\", StringType(), True),\n",
    "    StructField(\"DysonPONumber\", StringType(), True),\n",
    "    StructField(\"CustomerSKUNumber\", StringType(), True),\n",
    "    StructField(\"RoutingVersionId\", StringType(), True),\n",
    "    StructField(\"RoutingHeaderId\", StringType(), True),\n",
    "    StructField(\"ERPClosureStatus\", StringType(), True),\n",
    "    StructField(\"FeederReloadLockRequired\", StringType(), True),\n",
    "    StructField(\"MSDLockRequired\", StringType(), True),\n",
    "    StructField(\"Unit Price\", StringType(), True),\n",
    "    StructField(\"AllowCustomerRefNoRepetition\", StringType(), True),\n",
    "    StructField(\"Company\", StringType(), True),\n",
    "    StructField(\"Division\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plans_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"delimiter\",\"|\")\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/PlansShiftWise/PlansShiftWise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "results_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"False\")\\\n",
    "    .schema(resultsSchema)\\\n",
    "    .option(\"delimiter\",\",\")\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/Results/Results.csv\",inferSchema=True)\n",
    "routing_df = spark.read\\\n",
    "    .parquet(\"s3a://hackathon2023/data/OperationsManagement/RoutingStages/RoutingStages.parquet\",inferSchema=True)\n",
    "combined_df = results_df\\\n",
    "    .join(routing_df, [results_df.RoutingStageId == routing_df.id,results_df.WorkOrderId==routing_df.WorkOrderId], \"inner\")\\\n",
    "    .drop(routing_df.WorkOrderId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_orders_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "    .option(\"delimiter\",\"\\t\")\\\n",
    "    .schema(workOrdersSchema)\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/Workorders/Workorders.csv\",inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df\\\n",
    "    .join(work_orders_df, combined_df.WorkOrderId == work_orders_df.Id, \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df\\\n",
    "    .filter(combined_df.Surface == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df = combined_df.groupBy(\"ItemId\", \"SubWorkCenter\", \n",
    "                                hour(combined_df.LastModifiedDate).alias(\"Hour\"), \n",
    "                                date_format(combined_df.LastModifiedDate, \"yyyy-MM-dd\").alias(\"Date\")\n",
    "                               ).agg(countDistinct(\"BoardId\").alias(\"ActualQuantity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = actual_df.join(plans_df, \n",
    "                             (actual_df.ItemId == plans_df.ItemNo) & \n",
    "                             (actual_df.SubWorkCenter == plans_df.Station) & \n",
    "                             (actual_df.Hour == hour(plans_df.Hour)) & \n",
    "                             (actual_df.Date == date_format(plans_df.Date, \"yyyy-MM-dd\")), \n",
    "                             \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7=spark.read\\\n",
    "    .text(\"s3a://hackathon2023/data/OperationsManagement/Items/Items.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|C435Items-SB-0759...|\n",
      "|C530Items-SB-0686...|\n",
      "|C541Items-EE-0427...|\n",
      "|C749Items-EP-0256...|\n",
      "|C990Items-BC-0694...|\n",
      "|C828Items-BC-0261...|\n",
      "|C806Items-TS-0606...|\n",
      "|C995Items-SB-0233...|\n",
      "|C191Items-BC-0335...|\n",
      "|C451Items-EP-0879...|\n",
      "|C383Items-EP-0362...|\n",
      "|C465Items-BC-0305...|\n",
      "|C323Items-TS-0292...|\n",
      "|C125Items-TS-0574...|\n",
      "|C549Items-EP-0305...|\n",
      "|C901Items-EE-0827...|\n",
      "|C316Items-BC-0667...|\n",
      "|C559Items-SB-0192...|\n",
      "|C922Items-EE-0242...|\n",
      "|C268Items-BC-0923...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_7.show(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_7' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Use the 'udf' function to create a new column with the extracted values\u001b[39;00m\n\u001b[1;32m     15\u001b[0m extract_values_udf \u001b[38;5;241m=\u001b[39m udf(extract_values)\n\u001b[0;32m---> 16\u001b[0m df_7 \u001b[38;5;241m=\u001b[39m \u001b[43mdf_7\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructured_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, extract_values_udf(df_7\u001b[38;5;241m.\u001b[39mvalue))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create new columns based on the fields in the 'structured_data' struct\u001b[39;00m\n\u001b[1;32m     19\u001b[0m df_7 \u001b[38;5;241m=\u001b[39m df_7\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstructured_data.ID\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_7' is not defined"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import re\n",
    "from pyspark.sql.functions import udf, col, struct\n",
    "\n",
    "# Define a function to extract the values from the string\n",
    "def extract_values(s):\n",
    "  # Define the pattern to match\n",
    "  pattern = \"(.+?)UU-(.+?)nxklh(.+?)-(.+?)\\\\\\\\R\\$\\$(.+?)plantxi(.+?) (.+?)bHM(.+?)--(.+?)P011(.+?)MD(.+?)\"\n",
    "  # Use the 're' module to extract the values from the string\n",
    "  values = re.findall(pattern, s)[0]\n",
    "  # Return a struct with the values\n",
    "  return struct(*values)\n",
    "\n",
    "# Use the 'udf' function to create a new column with the extracted values\n",
    "extract_values_udf = udf(extract_values)\n",
    "df_7 = df_7.withColumn(\"structured_data\", extract_values_udf(df_7.value))\n",
    "\n",
    "# Create new columns based on the fields in the 'structured_data' struct\n",
    "df_7 = df_7.withColumn(\"ID\", col(\"structured_data.ID\"))\n",
    "df_7 = df_7.withColumn(\"Description\", col(\"structured_data.Description\"))\n",
    "df_7 = df_7.withColumn(\"Modality\", col(\"structured_data.Modality\"))\n",
    "df_7 = df_7.withColumn(\"Revision\", col(\"structured_data.Revision\"))\n",
    "df_7 = df_7.withColumn(\"BaseUOM\", col(\"structured_data.BaseUOM\"))\n",
    "df_7 = df_7.withColumn(\"Batch_Management\", col(\"structured_data.Batch_Management\"))\n",
    "df_7 = df_7.withColumn(\"SerialNumber_Profile\", col(\"structured_data.SerialNumber_Profile\"))\n",
    "df_7 = df_7.withColumn(\"ShelfLife\", col(\"structured_data.ShelfLife\"))\n",
    "df_7 = df_7.withColumn(\"ShelfLife_Date\", col(\"structured_data.ShelfLife_Date\"))\n",
    "df_7 = df_7.withColumn(\"MSD\", col(\"structured_data.MSD\"))\n",
    "df_7 = df_7.withColumn(\"Item_Category\", col(\"structured_data.Item_Category\"))\n",
    "df_7 = df_7.withColumn(\"MSLDetails\", col(\"structured_data.MSLDetails\"))\n",
    "df_7 = df_7.withColumn(\"MSLDetails\", col(\"structured_data.MSLDetails\"))\n",
    "\n",
    "\n",
    "# df_7=df_7.drop(\"value\")\n",
    "# df_7=df_7.drop(\"structured_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read in the \"plans shift wise\" data and select the relevant columns\n",
    "# Step 2: Read in the \"results\" and \"routing stages\" data and select the relevant columns\n",
    "# Combine the \"results\" and \"routing stages\" data using a inner join on the \"RoutingStageId\" and \"Id\" columns\n",
    "# Step 3: Read in the \"work orders\" data and select the relevant columns\n",
    "# Combine the output of step 2 with the \"work orders\" data using a left outer join on the \"WorkOrderId\" and \"Id\" columns\n",
    "# Filter out any records from combined_df with a value for the \"Surface\" column that is not equal to 1\n",
    "# Step 4: Calculate the actual production numbers by grouping and counting the unique \"BoardId\" values\n",
    "# across a combination of the \"ItemId\", \"SubWorkCenter\", \"LastModifiedDate\" (converted to the hour value),\n",
    "# and \"LastModifiedDate\" (converted to the date value) columns\n",
    "# Step 5: Combine the output of step 4 with the \"plans shift wise\" data using common columns\n",
    "# Step 6: Read in the \"items\" data and select the relevant columns\n",
    "    # items_df = spark.read.csv(\"items.csv\", header=True)\n",
    "# Combine the output of step 5 with the \"items\" data using a left outer join on the \"ItemId\" column\n",
    "    # final_df = combined_df.join(items_df, combined_df.ItemId == items_df.ItemId, \"left_outer\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

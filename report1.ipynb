{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.0.0\") \\\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", 'AKIA3AEXDSNEGXQERCGG') \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", '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') \\\n",
    "        .appName('Report 1 : Operations Management Report')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\"|\").load(\"s3a://hackathon2023/data/OperationsManagement/PlansShiftWise/PlansShiftWise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5=spark.read.parquet(\"s3a://hackathon2023/data/OperationsManagement/RoutingStages/RoutingStages.parquet\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4=spark.read.format(\"csv\").option(\"header\",\"False\").option(\"delimiter\",\",\").load(\"s3a://hackathon2023/data/OperationsManagement/Results/Results.csv\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6=spark.read.format(\"csv\").option(\"header\",\"false\").option(\"delimiter\",\"\\t\").load(\"s3a://hackathon2023/data/OperationsManagement/Workorders/Workorders.csv\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7=spark.read.text(\"s3a://hackathon2023/data/OperationsManagement/Items/Items.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"delimiter\",\"|\").load(\"s3a://hackathon2023/data/OperationsManagement/PlansShiftWise/PlansShiftWise.csv\")\n",
    "plans_df = plans_df.select(\"Quantity\", \"Date\", \"Hour\", \"Bay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = spark.read.format(\"csv\").option(\"header\",\"False\").option(\"delimiter\",\",\").load(\"s3a://hackathon2023/data/OperationsManagement/Results/Results.csv\",inferSchema=True).withColumnRenamed(\"_c5\",\"RoutingStageId\").withColumnRenamed('_c15','SubWorkCenter').withColumnRenamed('_c10','LastModifiedDate').withColumnRenamed('_c2','BoardId')\n",
    "routing_df = spark.read.parquet(\"s3a://hackathon2023/data/OperationsManagement/RoutingStages/RoutingStages.parquet\",inferSchema=True).withColumnRenamed(\"id\",\"Id\")\n",
    "combined_df = results_df.join(routing_df, results_df.RoutingStageId == routing_df.Id, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_orders_df = spark.read.format(\"csv\").option(\"header\",\"false\").option(\"delimiter\",\"\\t\").load(\"s3a://hackathon2023/data/OperationsManagement/Workorders/Workorders.csv\",inferSchema=True).withColumnRenamed('_c1','ItemId')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_orders_df=work_orders_df.withColumnRenamed(\"_c0\",\"Id\")\n",
    "combined_df = combined_df.join(work_orders_df, combined_df.WorkOrderId == work_orders_df.Id, \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.filter(combined_df.Surface == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_df = combined_df.groupBy(\"ItemId\", \"SubWorkCenter\", \n",
    "#                                 hour(combined_df.LastModifiedDate).alias(\"Hour\"), \n",
    "#                                 date_format(combined_df.LastModifiedDate, \"yyyy-MM-dd\").alias(\"Date\")\n",
    "#                                ).countDistinct(\"BoardId\").alias(\"ActualQuantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read in the \"plans shift wise\" data and select the relevant columns\n",
    "# Step 2: Read in the \"results\" and \"routing stages\" data and select the relevant columns\n",
    "# Combine the \"results\" and \"routing stages\" data using a inner join on the \"RoutingStageId\" and \"Id\" columns\n",
    "# Step 3: Read in the \"work orders\" data and select the relevant columns\n",
    "# Combine the output of step 2 with the \"work orders\" data using a left outer join on the \"WorkOrderId\" and \"Id\" columns\n",
    "# Filter out any records from combined_df with a value for the \"Surface\" column that is not equal to 1\n",
    "# Step 4: Calculate the actual production numbers by grouping and counting the unique \"BoardId\" values\n",
    "# across a combination of the \"ItemId\", \"SubWorkCenter\", \"LastModifiedDate\" (converted to the hour value),\n",
    "# and \"LastModifiedDate\" (converted to the date value) columns\n",
    "# Step 5: Combine the output of step 4 with the \"plans shift wise\" data using common columns\n",
    "# combined_df = actual_df.join(plans_df, \n",
    "#                              (actual_df.ItemId == plans_df.ItemNo) & \n",
    "#                              (actual_df.SubWorkCenter == plans_df.Station) & \n",
    "#                              (actual_df.Hour == HOUR(plans_df.Hour)) & \n",
    "#                              (actual_df.Date == WAVE_FORMAT_PCM(plans_df.Date, \"yyyy-MM-dd\")), \n",
    "#                              \"inner\")\n",
    "\n",
    "# Step 6: Read in the \"items\" data and select the relevant columns\n",
    "# items_df = spark.read.csv(\"items.csv\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType, TimestampType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.0.0\") \\\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", 'AKIA3AEXDSNEGXQERCGG') \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", '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') \\\n",
    "        .appName('Report 1 : Operations Management Report')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsSchema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"BoardId\", IntegerType(), True),\n",
    "    StructField(\"BatchId\", StringType(), True),\n",
    "    StructField(\"WorkOrderId\", StringType(), True),\n",
    "    StructField(\"RoutingStageId\", StringType(), True),\n",
    "    StructField(\"RoutingStageName\", StringType(), True),\n",
    "    StructField(\"Operator\", StringType(), True),\n",
    "    StructField(\"Deviation\", StringType(), True),\n",
    "    StructField(\"InspectionDate\", StringType(), True),\n",
    "    StructField(\"LastModifiedDate\", StringType(), True),\n",
    "    StructField(\"ReInspectionNeeded\", StringType(), True),\n",
    "    StructField(\"PreviouslySannedBoards\", StringType(), True),\n",
    "    StructField(\"RoutingStatus\", StringType(), True),\n",
    "    StructField(\"CavityID\", StringType(), True),\n",
    "    StructField(\"SubWorkCenter\", StringType(), True),\n",
    "    StructField(\"StationCode\", StringType(), True),\n",
    "    StructField(\"StationName\", StringType(), True),\n",
    "    StructField(\"TrayId\", StringType(), True),\n",
    "    StructField(\"AssetSubNodeId\", StringType(), True),\n",
    "    StructField(\"CollectionId\", StringType(), True),\n",
    "    StructField(\"Company\", StringType(), True),\n",
    "    StructField(\"Division\", StringType(), True),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workOrdersSchema=StructType([\n",
    "    StructField(\"Id\", StringType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True),\n",
    "    StructField(\"LineNo\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", StringType(), True),\n",
    "    StructField(\"Started\", StringType(), True),\n",
    "    StructField(\"StartDate\", StringType(), True),\n",
    "    StructField(\"EndDate\", StringType(), True),\n",
    "    StructField(\"EcnNo\", StringType(), True),\n",
    "    StructField(\"EcnQunatity\", StringType(), True),\n",
    "    StructField(\"EcnStatus\", StringType(), True),\n",
    "    StructField(\"ProductRevision\", StringType(), True),\n",
    "    StructField(\"PlannedStartDate\", StringType(), True),\n",
    "    StructField(\"PlannedEndDate\", StringType(), True),\n",
    "    StructField(\"Isblocked\", StringType(), True),\n",
    "    StructField(\"BlockedDate\", StringType(), True),\n",
    "    StructField(\"BlockedBy\", StringType(), True),\n",
    "    StructField(\"BatchProceedStatus\", StringType(), True),\n",
    "    StructField(\"WorkOrderClosureStatus\", StringType(), True),\n",
    "    StructField(\"ShortClosedQuantity\", StringType(), True),\n",
    "    StructField(\"CreationDate\", StringType(), True),\n",
    "    StructField(\"DysonPONumber\", StringType(), True),\n",
    "    StructField(\"CustomerSKUNumber\", StringType(), True),\n",
    "    StructField(\"RoutingVersionId\", StringType(), True),\n",
    "    StructField(\"RoutingHeaderId\", StringType(), True),\n",
    "    StructField(\"ERPClosureStatus\", StringType(), True),\n",
    "    StructField(\"FeederReloadLockRequired\", StringType(), True),\n",
    "    StructField(\"MSDLockRequired\", StringType(), True),\n",
    "    StructField(\"Unit Price\", StringType(), True),\n",
    "    StructField(\"AllowCustomerRefNoRepetition\", StringType(), True),\n",
    "    StructField(\"Company\", StringType(), True),\n",
    "    StructField(\"Division\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"delimiter\",\"|\")\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/PlansShiftWise/PlansShiftWise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"False\")\\\n",
    "    .schema(resultsSchema)\\\n",
    "    .option(\"delimiter\",\",\")\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/Results/Results.csv\",inferSchema=True)\n",
    "routing_df = spark.read\\\n",
    "    .parquet(\"s3a://hackathon2023/data/OperationsManagement/RoutingStages/RoutingStages.parquet\",inferSchema=True)\n",
    "combined_df = results_df\\\n",
    "    .join(routing_df, [results_df.RoutingStageId == routing_df.id,results_df.WorkOrderId==routing_df.WorkOrderId], \"inner\")\\\n",
    "    .drop(routing_df.WorkOrderId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_orders_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "    .option(\"delimiter\",\"\\t\")\\\n",
    "    .schema(workOrdersSchema)\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/Workorders/Workorders.csv\",inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df\\\n",
    "    .join(work_orders_df, combined_df.WorkOrderId == work_orders_df.Id, \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df\\\n",
    "    .filter(combined_df.Surface == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df = combined_df.groupBy(\"ItemId\", \"SubWorkCenter\", \n",
    "                                hour(combined_df.LastModifiedDate).alias(\"Hour\"), \n",
    "                                date_format(combined_df.LastModifiedDate, \"yyyy-MM-dd\").alias(\"Date\")\n",
    "                               ).agg(countDistinct(\"BoardId\").alias(\"ActualQuantity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = actual_df.join(plans_df, \n",
    "                             (actual_df.ItemId == plans_df.ItemNo) & \n",
    "                             (actual_df.SubWorkCenter == plans_df.Station) & \n",
    "                             (actual_df.Hour == hour(plans_df.Hour)) & \n",
    "                             (actual_df.Date == date_format(plans_df.Date, \"yyyy-MM-dd\")), \n",
    "                             \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7=spark.read\\\n",
    "    .text(\"s3a://hackathon2023/data/OperationsManagement/Items/Items.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import re\n",
    "from pyspark.sql.functions import udf, col, struct\n",
    "\n",
    "# Define a function to extract the values from the string\n",
    "def extract_values(s):\n",
    "  # Define the pattern to match\n",
    "  pattern = \"(.+?)UU-(.+?)nxklh(.+?)-(.+?)\\\\\\\\R\\$\\$(.+?)plantxi(.+?) (.+?)bHM(.+?)--(.+?)P011(.+?)MD(.+?)\"\n",
    "  # Use the 're' module to extract the values from the string\n",
    "  values = re.findall(pattern, s)[0]\n",
    "  # Return a struct with the values\n",
    "  return struct(*values)\n",
    "\n",
    "# Use the 'udf' function to create a new column with the extracted values\n",
    "extract_values_udf = udf(extract_values)\n",
    "df_7 = df_7.withColumn(\"structured_data\", extract_values_udf(df_7.value))\n",
    "\n",
    "# Create new columns based on the fields in the 'structured_data' struct\n",
    "df_7 = df_7.withColumn(\"ID\", col(\"structured_data.ID\"))\n",
    "df_7 = df_7.withColumn(\"Description\", col(\"structured_data.Description\"))\n",
    "df_7 = df_7.withColumn(\"Modality\", col(\"structured_data.Modality\"))\n",
    "df_7 = df_7.withColumn(\"Revision\", col(\"structured_data.Revision\"))\n",
    "df_7 = df_7.withColumn(\"BaseUOM\", col(\"structured_data.BaseUOM\"))\n",
    "df_7 = df_7.withColumn(\"Batch_Management\", col(\"structured_data.Batch_Management\"))\n",
    "df_7 = df_7.withColumn(\"SerialNumber_Profile\", col(\"structured_data.SerialNumber_Profile\"))\n",
    "df_7 = df_7.withColumn(\"ShelfLife\", col(\"structured_data.ShelfLife\"))\n",
    "df_7 = df_7.withColumn(\"ShelfLife_Date\", col(\"structured_data.ShelfLife_Date\"))\n",
    "df_7 = df_7.withColumn(\"MSD\", col(\"structured_data.MSD\"))\n",
    "df_7 = df_7.withColumn(\"Item_Category\", col(\"structured_data.Item_Category\"))\n",
    "df_7 = df_7.withColumn(\"MSLDetails\", col(\"structured_data.MSLDetails\"))\n",
    "df_7 = df_7.withColumn(\"MSLDetails\", col(\"structured_data.MSLDetails\"))\n",
    "\n",
    "\n",
    "# df_7=df_7.drop(\"value\")\n",
    "# df_7=df_7.drop(\"structured_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read in the \"plans shift wise\" data and select the relevant columns\n",
    "# Step 2: Read in the \"results\" and \"routing stages\" data and select the relevant columns\n",
    "# Combine the \"results\" and \"routing stages\" data using a inner join on the \"RoutingStageId\" and \"Id\" columns\n",
    "# Step 3: Read in the \"work orders\" data and select the relevant columns\n",
    "# Combine the output of step 2 with the \"work orders\" data using a left outer join on the \"WorkOrderId\" and \"Id\" columns\n",
    "# Filter out any records from combined_df with a value for the \"Surface\" column that is not equal to 1\n",
    "# Step 4: Calculate the actual production numbers by grouping and counting the unique \"BoardId\" values\n",
    "# across a combination of the \"ItemId\", \"SubWorkCenter\", \"LastModifiedDate\" (converted to the hour value),\n",
    "# and \"LastModifiedDate\" (converted to the date value) columns\n",
    "# Step 5: Combine the output of step 4 with the \"plans shift wise\" data using common columns\n",
    "# Step 6: Read in the \"items\" data and select the relevant columns\n",
    "    # items_df = spark.read.csv(\"items.csv\", header=True)\n",
    "# Combine the output of step 5 with the \"items\" data using a left outer join on the \"ItemId\" column\n",
    "    # final_df = combined_df.join(items_df, combined_df.ItemId == items_df.ItemId, \"left_outer\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_LOCAL_IP\"] = \"127.0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.0.0\") \\\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", 'AKIA3AEXDSNEGXQERCGG') \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", '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') \\\n",
    "        .appName('Report 1 : Operations Management Report')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3=spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"delimiter\",\"|\")\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/PlansShiftWise/PlansShiftWise.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5=spark.read\\\n",
    "    .parquet(\"s3a://hackathon2023/data/OperationsManagement/RoutingStages/RoutingStages.parquet\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType, TimestampType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsSchema = StructType([\n",
    "    StructField(\"Id\", IntegerType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"BoardId\", IntegerType(), True),\n",
    "    StructField(\"BatchId\", StringType(), True),\n",
    "    StructField(\"WorkOrderId\", StringType(), True),\n",
    "    StructField(\"RoutingStageId\", StringType(), True),\n",
    "    StructField(\"RoutingStageName\", StringType(), True),\n",
    "    StructField(\"Operator\", StringType(), True),\n",
    "    StructField(\"Deviation\", StringType(), True),\n",
    "    StructField(\"InspectionDate\", StringType(), True),\n",
    "    StructField(\"LastModifiedDate\", StringType(), True),\n",
    "    StructField(\"ReInspectionNeeded\", StringType(), True),\n",
    "    StructField(\"PreviouslySannedBoards\", StringType(), True),\n",
    "    StructField(\"RoutingStatus\", StringType(), True),\n",
    "    StructField(\"CavityID\", StringType(), True),\n",
    "    StructField(\"SubWorkCenter\", StringType(), True),\n",
    "    StructField(\"StationCode\", StringType(), True),\n",
    "    StructField(\"StationName\", StringType(), True),\n",
    "    StructField(\"TrayId\", StringType(), True),\n",
    "    StructField(\"AssetSubNodeId\", StringType(), True),\n",
    "    StructField(\"CollectionId\", StringType(), True),\n",
    "    StructField(\"Company\", StringType(), True),\n",
    "    StructField(\"Division\", StringType(), True),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4=spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"False\")\\\n",
    "    .schema(resultsSchema)\\\n",
    "    .option(\"delimiter\",\",\")\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/Results/Results.csv\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workOrdersSchema=StructType([\n",
    "    StructField(\"Id\", StringType(), True),\n",
    "    StructField(\"ItemId\", StringType(), True),\n",
    "    StructField(\"LineNo\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", StringType(), True),\n",
    "    StructField(\"Started\", StringType(), True),\n",
    "    StructField(\"StartDate\", StringType(), True),\n",
    "    StructField(\"EndDate\", StringType(), True),\n",
    "    StructField(\"EcnNo\", StringType(), True),\n",
    "    StructField(\"EcnQunatity\", StringType(), True),\n",
    "    StructField(\"EcnStatus\", StringType(), True),\n",
    "    StructField(\"ProductRevision\", StringType(), True),\n",
    "    StructField(\"PlannedStartDate\", StringType(), True),\n",
    "    StructField(\"PlannedEndDate\", StringType(), True),\n",
    "    StructField(\"Isblocked\", StringType(), True),\n",
    "    StructField(\"BlockedDate\", StringType(), True),\n",
    "    StructField(\"BlockedBy\", StringType(), True),\n",
    "    StructField(\"BatchProceedStatus\", StringType(), True),\n",
    "    StructField(\"WorkOrderClosureStatus\", StringType(), True),\n",
    "    StructField(\"ShortClosedQuantity\", StringType(), True),\n",
    "    StructField(\"CreationDate\", StringType(), True),\n",
    "    StructField(\"DysonPONumber\", StringType(), True),\n",
    "    StructField(\"CustomerSKUNumber\", StringType(), True),\n",
    "    StructField(\"RoutingVersionId\", StringType(), True),\n",
    "    StructField(\"RoutingHeaderId\", StringType(), True),\n",
    "    StructField(\"ERPClosureStatus\", StringType(), True),\n",
    "    StructField(\"FeederReloadLockRequired\", StringType(), True),\n",
    "    StructField(\"MSDLockRequired\", StringType(), True),\n",
    "    StructField(\"Unit Price\", StringType(), True),\n",
    "    StructField(\"AllowCustomerRefNoRepetition\", StringType(), True),\n",
    "    StructField(\"Company\", StringType(), True),\n",
    "    StructField(\"Division\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6=spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "    .option(\"delimiter\",\"\\t\")\\\n",
    "    .schema(workOrdersSchema)\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/Workorders/Workorders.csv\",inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7=spark.read\\\n",
    "    .text(\"s3a://hackathon2023/data/OperationsManagement/Items/Items.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "from pyspark.sql.functions import date_format\n",
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plans_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"delimiter\",\"|\")\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/PlansShiftWise/PlansShiftWise.csv\")\n",
    "plans_df = plans_df\\\n",
    "    .select(\"Quantity\", \"Date\", \"Hour\", \"Bay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"False\")\\\n",
    "    .schema(resultsSchema)\\\n",
    "    .option(\"delimiter\",\",\")\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/Results/Results.csv\",inferSchema=True)\n",
    "routing_df = spark.read\\\n",
    "    .parquet(\"s3a://hackathon2023/data/OperationsManagement/RoutingStages/RoutingStages.parquet\",inferSchema=True)\n",
    "combined_df = results_df\\\n",
    "    .join(routing_df, [results_df.RoutingStageId == routing_df.id,results_df.WorkOrderId==routing_df.WorkOrderId], \"inner\")\\\n",
    "    .drop(routing_df.WorkOrderId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_orders_df = spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\",\"false\")\\\n",
    "    .option(\"delimiter\",\"\\t\")\\\n",
    "    .schema(workOrdersSchema)\\\n",
    "    .load(\"s3a://hackathon2023/data/OperationsManagement/Workorders/Workorders.csv\",inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df\\\n",
    "    .join(work_orders_df, combined_df.WorkOrderId == work_orders_df.Id, \"left_outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df\\\n",
    "    .filter(combined_df.Surface == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_df = combined_df.groupBy(\"ItemId\", \"SubWorkCenter\", \n",
    "#                                 hour(combined_df.LastModifiedDate).alias(\"Hour\"), \n",
    "#                                 date_format(combined_df.LastModifiedDate, \"yyyy-MM-dd\").alias(\"Date\")\n",
    "#                                ).countDistinct(\"BoardId\").alias(\"ActualQuantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read in the \"plans shift wise\" data and select the relevant columns\n",
    "# Step 2: Read in the \"results\" and \"routing stages\" data and select the relevant columns\n",
    "# Combine the \"results\" and \"routing stages\" data using a inner join on the \"RoutingStageId\" and \"Id\" columns\n",
    "# Step 3: Read in the \"work orders\" data and select the relevant columns\n",
    "# Combine the output of step 2 with the \"work orders\" data using a left outer join on the \"WorkOrderId\" and \"Id\" columns\n",
    "# Filter out any records from combined_df with a value for the \"Surface\" column that is not equal to 1\n",
    "# Step 4: Calculate the actual production numbers by grouping and counting the unique \"BoardId\" values\n",
    "# across a combination of the \"ItemId\", \"SubWorkCenter\", \"LastModifiedDate\" (converted to the hour value),\n",
    "# and \"LastModifiedDate\" (converted to the date value) columns\n",
    "# Step 5: Combine the output of step 4 with the \"plans shift wise\" data using common columns\n",
    "# combined_df = actual_df.join(plans_df, \n",
    "#                              (actual_df.ItemId == plans_df.ItemNo) & \n",
    "#                              (actual_df.SubWorkCenter == plans_df.Station) & \n",
    "#                              (actual_df.Hour == HOUR(plans_df.Hour)) & \n",
    "#                              (actual_df.Date == WAVE_FORMAT_PCM(plans_df.Date, \"yyyy-MM-dd\")), \n",
    "#                              \"inner\")\n",
    "\n",
    "# Step 6: Read in the \"items\" data and select the relevant columns\n",
    "# items_df = spark.read.csv(\"items.csv\", header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

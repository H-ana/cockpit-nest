{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 20:53:39 WARN Utils: Your hostname, user-hp-pavilion-gaming-laptop-15-ec2xxx resolves to a loopback address: 127.0.1.1; using 192.168.184.92 instead (on interface wlo1)\n",
      "23/01/11 20:53:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/user/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/user/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-02a6ce93-1e3e-4f42-bbb6-77f6915933a7;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 418ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-02a6ce93-1e3e-4f42-bbb6-77f6915933a7\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/10ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 20:53:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 20:53:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from threading import Thread\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from bokeh.server.server import Server\n",
    "from bokeh.application import Application\n",
    "from bokeh.application.handlers.function import FunctionHandler\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from kafka import KafkaConsumer\n",
    "from tornado.ioloop import IOLoop\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, FloatType\n",
    "\n",
    "# Set access key and secret key\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIA3AEXDSNEGXQERCGG\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"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\"\n",
    "\n",
    "# Configure Spark to include the Kafka package\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"KafkaExample\").config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1\").getOrCreate()\n",
    "streamingInputDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"ec2-65-0-72-75.ap-south-1.compute.amazonaws.com:9092\").option(\"subscribe\", \"IOTTemperatureStream01\").option(\"startingOffsets\", \"earliest\").option(\"maxOffsetsPerTrigger\", 100).load()\n",
    "streamSchema = StructType([\n",
    "StructField(\"plant_name\", StringType(), True),\n",
    "StructField(\"lane_number\", StringType(), True),\n",
    "StructField(\"timestamp\", StringType(), True),\n",
    "StructField(\"temperature\", StringType(), True),\n",
    "StructField(\"component_info\", StructType(\n",
    "[\n",
    "StructField(\"component_type\", StringType(), True),\n",
    "StructField(\"component_manufacturer\", StringType(), True),\n",
    "]\n",
    "), True),\n",
    "])\n",
    "streamingDF = streamingInputDF.selectExpr(\"CAST(value AS STRING)\").select(from_json(col(\"value\"), streamSchema).alias(\"data\")).select(\"data.*\")\n",
    "def update_data(rdd):\n",
    "    data = rdd.collect()\n",
    "    current_time = datetime.now()\n",
    "    data = [d for d in data if (current_time - datetime.strptime(d['timestamp'], '%Y-%m-%d %H:%M:%S')).seconds/60 <= 30 and int(d['temperature']) > 50]\n",
    "    component_temperature = [(d['plant_name'], d['lane_number'], d['component_info']['component_type'], int(d['temperature'])) for d in data]\n",
    "    component_rdd = sc.parallelize(component_temperature)\n",
    "    component_rdd = component_rdd.map(lambda x: (x[0]+'-'+x[1], [(x[2], x[3])])).reduceByKey(lambda a,b : a+b).map(lambda x : (x[0], sorted(x[1], key = lambda y: y[1], reverse=True)))\n",
    "    component_rdd.foreachRDD(update_graph)\n",
    "def update_graph(rdd):\n",
    "    data = rdd.collect()\n",
    "    for d in data:\n",
    "        lane = d[0]\n",
    "        temperature = [x[1] for x in d[1]]\n",
    "        component = [x[0] for x in d[1]]\n",
    "        source = ColumnDataSource(data=dict(x=component, y=temperature, lane=lane))\n",
    "    p = figure(x_axis_label='Component Type', y_axis_label='Temperature', title='Component Temperature')\n",
    "    p.line(x='x', y='y', source=source)\n",
    "def make_document(doc):\n",
    "    doc.add_root(p)\n",
    "\n",
    "def bk_worker():\n",
    "    # create a new server\n",
    "    server = Server({'/bkapp': make_document}, io_loop=IOLoop(), allow_websocket_origin=[\"127.0.0.1:8000\"])\n",
    "    server.start()\n",
    "    server.io_loop.start()\n",
    "    thread = Thread(target=bk_worker)\n",
    "    thread.start()\n",
    "    streamingDF.writeStream.foreach(processData).start()\n",
    "    spark.streams.awaitAnyTermination()\n",
    "    server.stop()\n",
    "    thread.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
